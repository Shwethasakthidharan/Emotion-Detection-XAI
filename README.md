# Emotion-Detection-XAI

ðŸ“Œ Overview


This project aims to develop an accurate and transparent emotion detection system using multimodal fusion (image + video) and Explainable AI (XAI) techniques. Designed initially for classroom environments, the system helps educators understand students' emotional states in real-time while providing interpretable explanations for its predictions.

Key Features:


Multimodal Emotion Detection: Combines facial expressions and body language for higher accuracy.

Explainable AI (XAI): Uses SHAP (Shapley Additive Explanations) to explain model decisions.

Real-Time Processing: Works with live video feeds and uploaded files.

User-Friendly Interface: Built with Streamlit for easy interaction.

Educational Focus: Tailored for teachers and students to enhance engagement.

ðŸŽ¯ Objectives


Improve Emotion Detection Accuracy â€“ Uses CNNs to analyze facial expressions and body language.

Multimodal Fusion â€“ Combines image and video data for better emotional insights.

Explainable AI (XAI) â€“ Integrates SHAP to make AI decisions transparent.

Real-Time Processing â€“ Provides instant feedback for live classroom scenarios.

User-Friendly Interface â€“ Designed for educators and students with minimal technical expertise.

ðŸ”§ Technologies Used


Machine Learning: Convolutional Neural Networks (CNNs)

Explainable AI: SHAP (Shapley Additive Explanations)

Backend: Python, TensorFlow/Keras, OpenCV

Frontend: Streamlit

Data Processing: NumPy, Pandas, MediaPipe (for facial landmarks)

Usage:


Upload an image/video or use a live webcam feed.

The system will detect emotions (e.g., happiness, sadness, neutrality).

SHAP explanations will highlight which facial/body features influenced the prediction.

Teachers can use insights to adjust teaching methods; students get feedback on engagement.

ðŸ“Š Applications


Education: Monitor student engagement in real-time.


*Letâ€™s build AI that understands emotionsâ€”transparently and accurately! ðŸš€*
